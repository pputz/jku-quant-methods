---
title: "Central Limit Theorem"
subtitle: "Master Seminar Quantitative Research Methods"
author: Peter Putz
date: Institut für Organisation und Globale Managementstudien, JKU Linz
output: 
  ioslides_presentation:
    widescreen: true
    logo: ../../assets/img/jku.svg
    css: ../../assets/css/jku.css
    
---



## The Problem

Suppose we have measurements from a randomly drawn sample representing the number of defects per 10,000 units.

```
3  1  4  6  1  1  3  6  1   3
```

\
What can we infer about the population?


```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(ggplot2)
library(grid)
library(gridExtra)
```

## The Answer

We can infer the **population mean** within a defined (un)certainty.

\


"The 95% confidence interval for the mean defects per 10,000 units 
is 1.49 to 4.31."



## Fair Die Rolls

Probability Mass Function (PMF) of a fair die.

```{r fair, echo=FALSE, fig.height=3, fig.width=5}
df  <- data.frame(side = 1:6, prob = rep(1/6, 6))

ggplot(df, aes(side, prob)) + 
  geom_bar(stat = "identity", fill="#FF9999") +
  scale_y_continuous(limits=c(0, 0.3)) +
  scale_x_continuous(breaks=1:6) +
  theme_bw()
```



## Expected Value

- The **expected value** or **mean** of a random variable is the center of its distribution
- For discrete random variable $X$ with PMF $p(x)$, it is defined as follows
    $$
    E[X] = \sum_x xp(x).
    $$
    where the sum is taken over the possible values of $x$
- $E[X]$ represents the center of mass of a collection of locations and weights, $\{x, p(x)\}$




## Expected Value for Fair Die Rolls

```{r popFuction, echo=FALSE}
pop = function(pdf){
  mean  <- sum(pdf[1] * pdf[2])
  ex2  <- sum(pdf[1]^2 * pdf[2])
  var  <- ex2 - mean^2
  sd  <- sqrt(var)
  stats  <- list(mean=mean, var=var, sd=sd)
  stats
}
```

```{r expected, echo=FALSE, fig.height=3, fig.width=5}

ggplot(df, aes(side, prob)) + 
  geom_bar(stat = "identity", fill="#FF9999") +
  scale_x_continuous(breaks=1:6) +
  scale_y_continuous(limits=c(0, 0.3)) +
  geom_vline(xintercept = pop(df)$mean, color="blue", size=1) +
  annotate("text", x = 3.7, y = 0.25, hjust=0, label = "mu == 3.5", parse = TRUE) +
  theme_bw()
```


$E[X] =  1 \times \frac{1}{6} + 2 \times \frac{1}{6}+ 3 \times \frac{1}{6} + 4 \times \frac{1}{6} + 5 \times \frac{1}{6} + 6 \times \frac{1}{6} = 3.5$

## Variance

- The variance of a random variable is a measure of *spread*
- If $X$ is a random variable with mean $\mu$, the variance of $X$ is defined as

$$
\begin{align*}
Var(X) &= E[(X - \mu)^2]\\
       &= E[X^2] - E[X]^2
\end{align*}
$$
    
- The variance is the expected (squared) distance from the mean
- Densities with a higher variance are more spread out than densities with a lower variance

## Standard Deviation

- The square root of the variance is called the **standard deviation**
- The standard deviation has the same units as $X$

\

$$
\begin{align*}
\sigma &= \sqrt{Var(X)}\\
       &= \sqrt{E[X^2] - E[X]^2}
\end{align*}
$$


## Standard Deviation for Fair Die Rolls {.smaller}

```{r sd, echo=FALSE, fig.height=3, fig.width=5}
dfmu <- pop(df)$mean
dfsigma  <- pop(df)$sd

ggplot(df, aes(side, prob)) + 
  geom_bar(stat = "identity", fill="#FF9999") +
  scale_x_continuous(breaks=1:6) +
  scale_y_continuous(limits=c(0, 0.3)) +
  geom_vline(xintercept = dfmu, color="darkgrey", size=1) +
  annotate("segment", x = dfmu, xend = dfmu+dfsigma, y = 0.18, yend = .18,  
           colour = "blue", size=1,
           arrow = arrow(angle = 90, length = unit(0.06, "inches"), ends = "both")) +
  annotate("text", x = 3.3, y = 0.22, hjust=0, label = "mu == 3.5", angle = 90, parse = TRUE) +
  annotate("text", x = 3.8, y = 0.21, hjust=0, label = "sigma == 1.71", parse = TRUE) +
  xlab("") +
  theme_bw()
```



$E[X^2] = 1 ^ 2 \times \frac{1}{6} +  1 ^ 2 \times \frac{1}{6} + 3 ^ 2 \times \frac{1}{6} + 4 ^ 2 \times \frac{1}{6} + 5 ^ 2 \times \frac{1}{6} + 6 ^ 2 \times \frac{1}{6} \approx 15.17$

$Var(X) = E[X^2] - E[X]^2 = 15.5 - 3.5 ^ 2  \approx 2.92$

$SD(X) = \sqrt{2.92} \approx 1.71$




## Unfair Die Rolls {.smaller}

```{r, echo=FALSE, fig.height=3, fig.width=5}
duf  <- data.frame(side = 1:6, prob = c(2/6, 0 , 1/6, 1/6, 0 , 2/6))



dufmu <- pop(duf)$mean
dufsigma  <- pop(duf)$sd


ggplot(duf, aes(side, prob)) + 
  geom_bar(stat = "identity", fill="#FF9999") +
  scale_x_continuous(breaks=1:6) +
  scale_y_continuous(limits=c(0, 0.4)) +
  geom_vline(xintercept = dufmu, color="darkgrey", size=1) +
  annotate("segment", x = dufmu, xend = dfmu+dfsigma, y = .35, yend = .35,  
           colour = "blue", size=1,
           arrow = arrow(angle = 90, length = unit(0.06, "inches"), ends = "both")) +
  annotate("text", x = 3.3, y = 0.22, hjust=0, label = "mu == 3.5", angle = 90, parse = TRUE) +
  annotate("text", x = 3.8, y = 0.38, hjust=0, label = "sigma == 2.06", parse = TRUE) +
  xlab("") +
  theme_bw()
```


$E[X] = \mu = 1 \times \frac{2}{6} + 2 \times 0 + 3 \times \frac{1}{6} + 4 \times \frac{1}{6} + 5 \times 0 + 6 \times \frac{2}{6} = 3.5$
    
$E[X^2] = 1 ^ 2 \times \frac{2}{6} +  3 ^ 2 \times \frac{1}{6} + 4 ^ 2 \times \frac{1}{6} +  6 ^ 2 \times \frac{2}{6} = 16.5$

$Var(X) = \sigma ^ 2 = E[X^2] - E[X] ^ 2  = 4.25$

$SD(X) = \sqrt{4.25} \approx 2.06$



## Samples of n = 10 {.build}

Let's roll this unfair die ten times and write down the numbers.

```{r samples4, echo=FALSE, comment=NA}
n  <- 10
rep  <- 1000
set.seed(12345)
samples <- data.frame(matrix(sample(1:6, n * rep, replace = TRUE, 
                                    prob = c(2/6, 0, 1/6, 1/6, 0, 2/6)),
                             ncol = n))
print(samples[1,], row.names=F)
```

Let's do this a few more times (e.g. 1,000 times):

```{r echo=FALSE, comment=NA}
print(samples[2:10,], row.names=F)
```


## Sample Means n = 10

Next, calculate the sample means.

```{r samples4means, echo=FALSE, comment=NA}
samples$mean  <- apply(samples, 1, mean)

print(samples[1:10,])
```


## Distribution of Sample Means | n = 10; rep = 1000

```{r, echo=FALSE, fig.height=3, fig.width=5}
ggplot(samples, aes(mean)) + 
  geom_histogram(fill="#FF9999", binwidth = 0.2)
```

$\bar{x} = `r mean(samples$mean)`$; $S = `r sd(samples$mean)`$; $S/\sqrt{n} =`r sd(samples$mean) * sqrt(n)`$


## Monte Carlo Simulation



## Simulation of Sample Means

```{r, echo = FALSE, fig.width=9, fig.height = 5.5, warning=FALSE}

h <- list()
for (n in c(1, 4, 10, 100)){
  samples <- matrix(sample(1:6, n * 1000, replace = TRUE, 
                        prob = c(2/6, 0, 1/6, 1/6, 0, 2/6)), ncol = n)
  samples  <- data.frame(samples)
  samples$mean <- apply(samples, 1, mean)
  
  h[[n]] <- ggplot(samples, aes(mean)) + 
    geom_histogram(fill="#FF9999", binwidth = 0.2, center=0) +
    scale_x_continuous(limits = c(0.8, 6.2), breaks=c(1:6)) +
    geom_vline(xintercept = dufmu, color="blue", size=1) +
    annotate("text", x = 3.5, y =0 , hjust=1, label = "mu", parse = TRUE) +
    labs(x ="", title = paste("n =", n)) +
    theme_bw()
}

grid.arrange(h[[1]], h[[4]], h[[10]], h[[100]], ncol = 2)
```


## Observations


>- The sample means are distributed around the population mean

>- The variance deceases as n gets greater

>- The distribution looks like a bell curve



## CLT Formal

Lindeberg–Lévy CLT: 

Suppose $\{X_1, X_2, ...\}$ is a sequence of i.i.d. random variables with $E[Xi_] = \mu$ and $Var[X_i] = \sigma^2 < \infty$.

Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n − \mu)$ converge in distribution to a normal $N(0, \sigma^2)$

$$
\sqrt{n} \left( \left(\frac{1}{n}\sum_{i=1}^{n} X_i \right) - \mu\right) 
\overset{d}{\to} N(0, \sigma^2)
$$


## Practical Application

Estimate of the confidence interval the mean:

$$
\bar{x} \pm z_{\alpha/2} \frac{s}{\sqrt{n}}
$$

Suppose the significance level $\alpha = 0.05$:

In 95% of random samples of size $n$ the actual population mean lays within 
the boundaries of the confidence interval.

For small samples we need to use the t distribution:

$$
\bar{x} \pm t_{\alpha/2,\ n-1} \frac{s}{\sqrt{n}}
$$



## Example from the Beginning

Suppose we have have measurements from a randomly drawn sample:

```
3  1  4  6  1  1  3  6  1   3
```

```{r}
x <- c(3, 1, 4, 6, 1, 1, 3, 6, 1, 3)
mean(x)
mean(x) + c(-1, 1) * qt(.975, 9) * sd(x)/sqrt(10)
```




## Simulation

```{r echo=FALSE}
rep <- 1e3        # number of simulations
n <- 40              # sample size
prob  <- c(2/6, 0 , 1/6, 1/6, 0 , 2/6)             # 
set.seed(12345)       # seed for rexp

dat <- data.frame(t(apply(matrix(sample(1:6, n * rep, replace = TRUE, 
                                    prob = prob), rep), 1,  
             function(x) c(mean(x), var(x)))))
colnames(dat) <- c("mean", "var")

dat$ul <- dat$mean+1.96*sqrt(dat$var)/sqrt(n) # upper limit for x bar
dat$ll <- dat$mean-1.96*sqrt(dat$var)/sqrt(n) # lower limit for x bar
dat$covered <- (3.5 > dat$ll & 3.5 < dat$ul) # is E(X) between ll & ul?
CV <- mean(dat$covered) 
CV
```

```{r, echo=FALSE}
dat1 <- dat[101:200, ]
ggplot(dat1, aes(row(dat1[1]), mean, ymin = ul, ymax=ll, colour = covered)) +
    geom_linerange() +
    geom_hline(yintercept=3.5) +
    theme_bw(base_size = 14) +
    theme(legend.position = "none") +
    xlab("Simulations") +
    ylab("95\\% Confidence Intervals") +
    annotate("text", x = 92, y = 3.7, label = "true mean", colour = "grey20")
```


## Interpretation

\
\

The 95% refers to the fact that if one were to repeatedly get samples of size n, about 95% of the intervals obtained would contain $\mu$. 



## Example



```{r}
ssl  <- iris[iris$Species == "setosa", 1]

mn  <- mean(ssl)
s  <- sd(ssl)
n  <- length(ssl)

error  <- qt(.975, n-1) * s / sqrt(n)
SEM  <- mn + c(-1, 1) * error
```

The 95% confidence interval for the mean sepal length of  *iris setosa* is `r round(mn, 2)` ± `r round(error, 2)` cm.

## Alternative Computation

```{r}
t.test(ssl, conf.level = 0.95)
```

## Nomenclature

Greek letters refer to population statistics, Latin letters to sample statistics.

- Suppose $X_i$ are iid with mean $\mu$ and variance $\sigma^2$
- $S^2$ estimates $\sigma^2$
- $S / \sqrt{n}$ estimates $\sigma / \sqrt{n}$ the standard error of the mean
- $S / \sqrt{n}$ is called the sample standard error (of the mean)


## Acknowledgements

\
\
\

<div class="centered">
This class and many slides were inspired by:

**Brian Caffo, Jeff Leek, and Roger Peng**   
Department of Biostatistics   
Johns Hopkins Bloomberg School of Public Health   

and the  

[Coursera Data Science Specialization Program](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop)

</div>


