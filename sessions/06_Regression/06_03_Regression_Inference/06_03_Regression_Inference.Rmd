---
title: "Inference in Regression"
subtitle: "Master Seminar Quantitative Research Methods"
author: Peter Putz
date: Institut f√ºr Organisation und Globale Managementstudien, JKU Linz
output: 
  ioslides_presentation:
    widescreen: true
    logo: ../../assets/img/jku.svg
    css: ../../assets/css/jku.css
    
---


<style>
table.rmdtable th {
  color: white;
  font-size: 18px;
  background: -webkit-gradient(linear, 50% 0%, 50% 100%, color-stop(40%, #1e786d), color-stop(80%, #144f48)) no-repeat;
  background: -webkit-linear-gradient(top, #1e786d 40%, #144f48 80%) no-repeat;
  background: -moz-linear-gradient(top, #1e786d 40%, #144f48 80%) no-repeat;
  background: -o-linear-gradient(top, #1e786d 40%, #144f48 80%) no-repeat;
  background: linear-gradient(top, #1e786d 40%, #144f48 80%) no-repeat;
}
</style>



# Regression Coefficients | Hypotheses Tests and Confidence Intervals


## Recall Our Model and Fitted Values

- Consider the model
$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
$$

- $\hat \beta_0 = \bar Y - \hat \beta_1 \bar X$
- $\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)}$.  
- $\epsilon \sim N(0, \sigma^2)$. 

- We assume that the true model is known.



## What We Try to Accomplish

Perform hypothesis tests:

- $H_0$: $\beta_0 = 0$; $H_a$: $\beta_0 \neq 0$
- $H_0$: $\beta_1 = 0$; $H_a$: $\beta_1 \neq 0$


\


Calculate confidence intervals for

- $\beta_0$
- $\beta_1$


## Review

Statistics like $\frac{\hat \theta - \theta}{\hat \sigma_{\hat \theta}}$ often have the following properties:

1. Is normally distributed and has a finite sample Student's T distribution if the
variance is replaced with a sample estimate (under normality assumptions).
3. Can be used to test $H_0 : \theta = \theta_0$ versus $H_a : \theta >, <, \neq \theta_0$.
4. Can be used to create a confidence interval for $\theta$ via $\hat \theta \pm Q_{1-\alpha/2} \hat 
\sigma_{\hat \theta}$,  
where $Q_{1-\alpha/2}$ is the relevant quantile from either a normal or T distribution.



## Results

* $\sigma_{\hat \beta_1}^2 = Var(\hat \beta_1) = \sigma^2 / \sum_{i=1}^n (X_i - \bar X)^2$
* $\sigma_{\hat \beta_0}^2 = Var(\hat \beta_0)  = \left(\frac{1}{n} + \frac{\bar X^2}{\sum_{i=1}^n (X_i - \bar X)^2 }\right)\sigma^2$
* In practice, $\sigma$ is replaced by its estimate.
* It's probably not surprising that under iid Gaussian errors
$$
\frac{\hat \beta_j - \beta_j}{\hat \sigma_{\hat \beta_j}}
$$
follows a $t$ distribution with $n-2$ degrees of freedom and a normal distribution for large $n$.
* This can be used to create confidence intervals and perform
hypothesis tests.



## Example Diamond Data Set

```{r, message=FALSE, warning=FALSE}
library(UsingR); data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)               # slope
beta0 <- mean(y) - beta1 * mean(x)               # intercept
e <- y - beta0 - beta1 * x                       # residuals
sigma <- sqrt(sum(e^2) / (n-2))                  # residual variation 
ssx <- sum((x - mean(x))^2)                      # sum of squared deviations from mean X
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma 
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
```


## Example cont'd

Hypothesis Test: $H_0: \beta_n = 0$; $H_a: \beta_n \neq 0$

```{r}
coefTable
fit <- lm(y ~ x); 
summary(fit)$coefficients
```


## Getting a Confidence Interval

```{r}
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
confint(fit, "x", level = 0.95) /10
```
With 95% confidence, we estimate that a 0.1 carat increase in
diamond size results in a SIN$ `r round((sumCoef[2,1] - qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)` to `r round((sumCoef[2,1] + qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)` increase in price.



# Prediction of Outcomes


## Prediction of Outcomes

- Consider predicting $Y$ at a value of $X$
    e.g. predicting the price of a diamond given it's size in carat
- The obvious estimate for prediction at point $x_0$ is 
$$
\hat \beta_0 + \hat \beta_1 x_0
$$
- A standard error is needed to create a prediction interval.
- There's a distinction between intervals for the regression
  line at point $x_0$ and the prediction of what a $y$ would be
  at point $x_0$. 
- Line at $x_0$ se, $\hat \sigma\sqrt{\frac{1}{n} +  \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}$
- Prediction interval se at $x_0$, $\hat \sigma\sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}$



## Plotting the Prediction Intervals {.centered}

```{r, fig.height=5, fig.width=7, echo = FALSE, results='hide'}
library(ggplot2)
newx = data.frame(x = seq(min(x), max(x), length = 100))
p1 = data.frame(predict(fit, newdata= newx,interval = ("prediction")))
p2 = data.frame(predict(fit, newdata = newx,interval = ("confidence")))
p1$interval = "prediction"
p2$interval = "confidence"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"

g <- ggplot(dat, aes(x = x, y = y))
g <- g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = rev(interval)), alpha = 1)
g <- g + scale_fill_hue(h = c(0, 360) + 15, c = 100, l = 75, h.start = 180)
g <- g + geom_line()
g <- g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 3, alpha = 0.5)
g <- g + theme_classic()
g <- g + labs(x = "Mass (carat)", y = "Price (SIN $)", fill = "Interval")
g
```



## Prediction of Outcomes in R {.smaller}

```{r}
newsize <- data.frame(x = c(0.16, 0.27, 0.34))
predict(fit, newsize)
predict(fit, newsize, interval = "prediction")
predict(fit, newsize, interval = "confidence")
```



## Discussion

- Both intervals have varying widths.  
    Least width at the mean of the Xs.
- We are quite confident in the regression line, so that 
  interval is very narrow.  
    If we knew $\beta_0$ and $\beta_1$ this interval would have zero width.
- The prediction interval must incorporate the variabilibity
  in the data around the line.  
    Even if we knew $\beta_0$ and $\beta_1$ this interval would still have width.




## Acknowledgements {.centered}

\
\
\

This class and many slides were inspired by:

**Brian Caffo, Jeff Leek, and Roger Peng**   
Department of Biostatistics   
Johns Hopkins Bloomberg School of Public Health   

and the  

[Coursera Data Science Specialization Program](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop)



